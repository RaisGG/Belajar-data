# -*- coding: utf-8 -*-
"""Project1_rev.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KlqjwpftYvaCttECBS61XEKPD6VY50_c

###**DOMAIN PROYEK**

Domain yang kita pilih pada projek ini adalah Ekonomi dan bisnis. Khususnya dalam mengantisipasi kebangkrutan suatu perusahaan.

Istilah bangkrut atau pailit dinyatakan sebagai ketidakmampuan suatu perusahaan untuk membayar utang-utangnya kepada para krediturnya. Mengetahui kemungkinan kebangkutan perusahaan merupakan hal yang penting bagi pengurus perusahaan, investor perusahaan dan bahkan masyarakat. Oleh karena itu, kita harus dapat melakukan prediksi kemungkinan kebangkrutan suatu perusahaan sebelum perusahaan itu benar-benar bangkrut. 

Dengan ini, kita akan mencoba untuk mengembangkan model algoritma machine learning yang dapt melakukan prediksi apakah perusahaan tersebut berpotensi bangkrut atau tidak dengan laporan keuangan dan rasio keuangannya.

Dataset yang akan kita gunakan adalah Company Bankruptcy Prediction dataset yang dapat didownload dari [kaggle](https://archive.ics.uci.edu/ml/datasets/Taiwanese+Bankruptcy+Prediction/) atau [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Taiwanese+Bankruptcy+Prediction/)

### **BUSINESS UNDERSTANDING**

Menjaga stabilitas suatu perusahaan adalah hal yang sangat penting untuk kelangsungan suatu perusahaan. Perusahaan dengan rasio keuangan yang stabil akan menjaga perusahaan tersebut tetap bertahan. Semakin baik laporan keuangan dan rasio keuangan kas sebuah perusahaan, semakin baik pula stabilitas suatu perusahaan. Sehingga menjaga rasio keuangan perusahaan adalah yang yang harus dilakukan. Bahkan banyak sekali perushaan yang harus melakukan utang untuk dapat menjaga eksistensi perusahaan.

Kebangkrutan atau ketidakmampuan suatu perusahaan untuk membayar utang-utangnya kepada para krediturnya, merupakan hal yang paling ditakuti oleh perusahaan. Ketika suatu perusahaan telah dinyatakan bangkrut dan tidak dapat membayar utang-utangnya, akan sangat sulit bagi perusahaan tersebut untuk bangkit Kembali. Sehingga penanggulangan kebangkrutan adalah hal yang mutlak dilakukan oleh perusahaan. Pada penelitian kali ini, kita akan mengunakan machine learning untuk melakukan prediksi kebangkrutan suatu perusahaan, sehingga dapat mengantisinya sedini mungkin.

###**PROBLEM STATEMENT**

Berdasarkan kondisi yang telah diuraikan sebelumnya, saya akan mengembangkan sebuah sistem prediksi untuk menjawab permasalahan.

Bagaimana kemungkinan kebangkrutan perushaan dengan dengan laporan keuangan dan rasio keuangannya?

###**GOALS**

Untuk  menjawab pertanyaan tersebut, dibuat predictive modelling dengan tujuan atau goals sebagai berikut.

Membuat model machine learning yang dapat memprediksi kebangkrutan perusahaan dengan menggunakan fitur yang disediakan sehingga mendapatkan informasi yang lebih jelas tentang masa depan perusahaan.

###**SOLUTION STATEMENT**

Untuk prediksi ini, 3 algoritma machine learning yang berbeda akan digunakan dalam studi kasus ini. Algoritma yang digunakan adalah sebagai berikut:

1. Logistic regression: Ini adalah metode klasifikasi pembelajaran mesin dan mencoba memprediksi variabel dependen kategoris yang dikodekan sebagai biner (1 – yes, berhasil dll., 0 – no, tidak berhasil, dll.). 

2. Support Vector Machine (SVM): Support Vector Machine (SVM) merupakan salah satu metode dalam supervised learning yang biasanya digunakan untuk klasifikasi (seperti Support Vector Classification) dan regresi (Support Vector Regression). Dalam pemodelan klasifikasi, SVM memiliki konsep yang lebih matang dan lebih jelas secara matematis dibandingkan dengan teknik-teknik klasifikasi lainnya. SVM juga dapat mengatasi masalah klasifikasi dan regresi dengan linear maupun non linear.

3. Random Forest: Random Forest adalah salah satu metode berbasis klasifikasi dan regresi dimana terdapat proses agregasi decision tree. Oleh karena itu, prinsip dasar random forest mirip dengan decision tree. Masing-masing decision tree akan menghasilkan output yang bisa saja berbeda-beda. Nah, random forest ini akan melakukan voting untuk menentukan hasil mayoritas dari semua decision tree. Bedanya, random forest akan memberikan output berupa mayoritas hasil dari semua decision tree. Algoritma ini memberikan akurasi yang bagus dalam klasifikasi, dapat menangani data training yang jumlahnya besar, dan juga efektif untuk mengatasi data yang tidak lengkap.

### **DATA UNDERSTANDING**

Dataset dikumpulkan dari Jurnal Ekonomi Taiwan untuk tahun 1999 sampai 2009. Kepailitan perusahaan didefinisikan berdasarkan peraturan bisnis dari Bursa Efek Taiwan.Data memiliki dimensi 6819 x 96, yang berarti dataset memiliki 96 kolom dan 6819 baris.

Sumber dataset [Company Bankruptcy Prediction]( https://www.kaggle.com/fedesoriano/company-bankruptcy-prediction)

The data was obtained from [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Taiwanese+Bankruptcy+Prediction)

Berikut ini merupakan daftar kolom yang terdapat dalam dataset(Y = Output feature, X = Input features)

* Y - Bankrupt?: Class label
* X1 - ROA(C) before interest and depreciation before interest: Return On Total Assets(C)
* X2 - ROA(A) before interest and % after tax: Return On Total Assets(A)
* X3 - ROA(B) before interest and depreciation after tax: Return On Total Assets(B)
* X4 - Operating Gross Margin: Gross Profit/Net Sales
* X5 - Realized Sales Gross Margin: Realized Gross Profit/Net Sales
* X6 - Operating Profit Rate: Operating Income/Net Sales
* X7 - Pre-tax net Interest Rate: Pre-Tax Income/Net Sales
* X8 - After-tax net Interest Rate: Net Income/Net Sales
* X9 - Non-industry income and expenditure/revenue: Net Non-operating Income Ratio
* X10 - Continuous interest rate (after tax): Net Income-Exclude Disposal Gain or Loss/Net Sales
* X11 - Operating Expense Rate: Operating Expenses/Net Sales
* X12 - Research and development expense rate: (Research and Development Expenses)/Net Sales
* X13 - Cash flow rate: Cash Flow from Operating/Current Liabilities
* X14 - Interest-bearing debt interest rate: Interest-bearing Debt/Equity
* X15 - Tax rate (A): Effective Tax Rate
* X16 - Net Value Per Share (B): Book Value Per Share(B)
* X17 - Net Value Per Share (A): Book Value Per Share(A)
* X18 - Net Value Per Share (C): Book Value Per Share(C)
* X19 - Persistent EPS in the Last Four Seasons: EPS-Net Income
* X20 - Cash Flow Per Share
* X21 - Revenue Per Share (Yuan ¥): Sales Per Share
* X22 - Operating Profit Per Share (Yuan ¥): Operating Income Per Share
* X23 - Per Share Net profit before tax (Yuan ¥): Pretax Income Per Share
* X24 - Realized Sales Gross Profit Growth Rate
* X25 - Operating Profit Growth Rate: Operating Income Growth
* X26 - After-tax Net Profit Growth Rate: Net Income Growth
* X27 - Regular Net Profit Growth Rate: Continuing Operating Income after Tax Growth
* X28 - Continuous Net Profit Growth Rate: Net Income-Excluding Disposal Gain or Loss Growth
* X29 - Total Asset Growth Rate: Total Asset Growth
* X30 - Net Value Growth Rate: Total Equity Growth
* X31 - Total Asset Return Growth Rate Ratio: Return on Total Asset Growth
* X32 - Cash Reinvestment %: Cash Reinvestment Ratio
* X33 - Current Ratio
* X34 - Quick Ratio: Acid Test
* X35 - Interest Expense Ratio: Interest Expenses/Total Revenue
* X36 - Total debt/Total net worth: Total Liability/Equity Ratio
* X37 - Debt ratio %: Liability/Total Assets
* X38 - Net worth/Assets: Equity/Total Assets
* X39 - Long-term fund suitability ratio (A): (Long-term Liability+Equity)/Fixed Assets
* X40 - Borrowing dependency: Cost of Interest-bearing Debt
* X41 - Contingent liabilities/Net worth: Contingent Liability/Equity
* X42 - Operating profit/Paid-in capital: Operating Income/Capital
* X43 - Net profit before tax/Paid-in capital: Pretax Income/Capital
* X44 - Inventory and accounts receivable/Net value: (Inventory+Accounts Receivables)/Equity
* X45 - Total Asset Turnover
* X46 - Accounts Receivable Turnover
* X47 - Average Collection Days: Days Receivable Outstanding
* X48 - Inventory Turnover Rate (times)
* X49 - Fixed Assets Turnover Frequency
* X50 - Net Worth Turnover Rate (times): Equity Turnover
* X51 - Revenue per person: Sales Per Employee
* X52 - Operating profit per person: Operation Income Per Employee
* X53 - Allocation rate per person: Fixed Assets Per Employee
* X54 - Working Capital to Total Assets
* X55 - Quick Assets/Total Assets
* X56 - Current Assets/Total Assets
* X57 - Cash/Total Assets
* X58 - Quick Assets/Current Liability
* X59 - Cash/Current Liability
* X60 - Current Liability to Assets
* X61 - Operating Funds to Liability
* X62 - Inventory/Working Capital
* X63 - Inventory/Current Liability
* X64 - Current Liabilities/Liability
* X65 - Working Capital/Equity
* X66 - Current Liabilities/Equity
* X67 - Long-term Liability to Current Assets
* X68 - Retained Earnings to Total Assets
* X69 - Total income/Total expense
* X70 - Total expense/Assets
* X71 - Current Asset Turnover Rate: Current Assets to Sales
* X72 - Quick Asset Turnover Rate: Quick Assets to Sales
* X73 - Working capitcal Turnover Rate: Working Capital to Sales
* X74 - Cash Turnover Rate: Cash to Sales
* X75 - Cash Flow to Sales
* X76 - Fixed Assets to Assets
* X77 - Current Liability to Liability
* X78 - Current Liability to Equity
* X79 - Equity to Long-term Liability
* X80 - Cash Flow to Total Assets
* X81 - Cash Flow to Liability
* X82 - CFO to Assets
* X83 - Cash Flow to Equity
* X84 - Current Liability to Current Assets
* X85 - Liability-Assets Flag: 1 if Total Liability exceeds Total Assets, 0 otherwise
* X86 - Net Income to Total Assets
* X87 - Total assets to GNP price
* X88 - No-credit Interval
* X89 - Gross Profit to Sales
* X90 - Net Income to Stockholder's Equity
* X91 - Liability to Equity
* X92 - Degree of Financial Leverage (DFL)
* X93 - Interest Coverage Ratio (Interest expense to EBIT)
* X94 - Net Income Flag: 1 if Net Income is Negative for the last two years, 0 otherwise
* X95 - Equity to Liability

### **DATA PREPARATION**

Pada bagian ini, dilakukan pengolahan data sebelum data dapat dilakukan modelling.Saya menyorot dua proses data preparation yang utama dalam proses ini, yaitu melakukan oversampling, dan reduksi dimensi.

1. Langkah pertama adalah melakukan Import Library yang dibutuhkan dalam pengolahan data.
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np #Linear Aljabra
import pandas as pd # data processing,
import matplotlib #digunakan untuk memvisualisasikan data
import matplotlib.pyplot as plt
import seaborn as sns #untuk melakukan visualisai agar lebih menarik
# %matplotlib inline

#Import library untuk melakukan komparasi matrix
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import f1_score
from sklearn.metrics import recall_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import precision_score

"""2. Dataset memiliki nama 'data.csv', 
Kita dapat membaca dataset dengan menggunakan fungsi berikut
"""

df = pd.read_csv('data.csv')

"""3. Selanjutnya kita dapat melihat gambaran dasar dataset, kita dapat meilhat lima data pertama dari dataset dengan fungsi berikut."""

df.head()

"""4. Kita coba untuk melihat dimensi dari dataset"""

df.shape

"""Dapat kita ketahui bahwa dataset terdiri dari **6819** baris dan **96** kolom, jumlah kolom yang terlalu banyak akan berdampak pad kualitas dari model yang kita buat. Sehingga selanjutnya kita akan coba melakukan reduksi dimensi.

5. Selanjutnya kita akan mengecek apakah terdapat data yang kosong
"""

df.isnull().sum()

"""Dataset yang kita miliki, tidak memiliki nilai data yang kosong, sehingga dapat kita abaikan

6. Selanjutnya kita mengecek apakah terdapat data yang mengandung duplikat
"""

df.duplicated().sum()

"""Dataset yang kita miliki, tidak memiliki nilai data yang duplikat, sehingga dapat kita abaikan

7. Mengecek informasi pada dataset dengan fungsi info()
"""

df.info()

"""Dapat diketahui bahwa tipe dataset terdiri dari float64 dan int64

8. Kita akan mengecek total masing-masing variabel target
"""

df['Bankrupt?'].value_counts()

"""9. Kita coba untuk melihat perbandingan jumlah variabel dengan menggunakan grafik"""

sns.countplot(x=df['Bankrupt?'])
plt.title('Target feature - Bankrupt?')

"""Total masing-masing variabel sangat tidak seimbang, hal ini sangat buruk bagi proses modeling kedepannya. Sehingga kita harus melakukan oversampling

10. Melihat hubungan tiap fitur dengan Matrix korelasi
"""

plt.figure(figsize=(17,17))
sns.heatmap(df.corr(), annot=False, cmap='coolwarm')
plt.show()

"""Kita dapat dengan jelas melihat bahwa beberapa fitur sangat terkait satu sama lain. kita akan coma mereduksinya.

11. kita akan memisahkan fitur dependent dan fitur independent, dimana fitur bangkrup? sebagai fitur target atau independent
"""

X=df.drop('Bankrupt?', axis=1) #X sebagai fitur dependent, sedangkan y debagai independent
y=df['Bankrupt?']

"""12. Oversampling dengan SMOTE

Synthetic Minority Oversampling Technique(SMOTE) adalah salah satu metode oversampling yang bekerja dengan meningkatkan jumlah kelas positif melalui replikasi data secara acak, sehingga jumlah data positif sama dengan data negatif. 

Dataset mengandung target value yang tidak seimbang, dimana perbandingan jumlah perusahaan yang bertahan dengan perusahaan yang bangrut, sangat tidak berimbang (6599 berbanding 220) sehingga akan mengurangi kualitas dari modelling. 

"""

from imblearn.over_sampling import SMOTE

"""Seperti yang kita ketahui sebelumnya, kita ketahui bahwa Target Value dangat tidak seimbang, sehingga perlu kita perlu menyeimbangkan Target value dengan menggunakan SMOTE."""

oversample = SMOTE()
X,y=oversample.fit_resample(X,y)

"""Mari kita lihat perbandingan jumlah target value"""

sns.countplot(x=y)

"""Dapat kita ketahui bahwa jumlah masing masing target value telah seimbang

13. Selanjutnya kita dapat melakukan Spliting data dengan menggunakan train_test_split yang terdapat dalam sklearn
"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=97, test_size=0.2)

"""14. Selanjutnya adalah melakukan standarisasi variabel"""

from sklearn.preprocessing import StandardScaler

Scaled_X_train = StandardScaler().fit_transform(X_train)
Scaled_X_test = StandardScaler().fit_transform(X_test)

"""Melihat dimensi dari hasil splitting dataset"""

print(X_train.shape,y_train.shape)
print(X_test.shape,y_test.shape)

"""15. Reduksi dimensi dengan menggunakan teknik PCA

Teknik reduksi (pengurangan) dimensi adalah prosedur yang mengurangi jumlah fitur dengan tetap mempertahankan informasi pada data. Teknik pengurangan dimensi yang paling populer adalah Principal Component Analysis atau disingkat menjadi PCA. Ia adalah teknik reduksi untuk mengubah fitur asli menjadi kumpulan fitur lain yang tidak berkorelasi linier, disebut komponen utama (PC).

Dataset yang diolah, memiliki jumlah feature yang terlalu banyak, yaitu sebesar 96 fitur. Padahal, banyak fitur yang memiliki tingkat korelasi yang cukup tinggi. Jumlah fitur yang terlalu banyak akan menghambat proses modelling dari dataset. Sehingga kita perlu untuk mereduksi dimensidari dataset dengan menggunakan Teknik PCA.
"""

from sklearn.decomposition import PCA
pc = PCA(n_components=95)
X_train_pc=pc.fit_transform(Scaled_X_train)
PC_df_train=pd.DataFrame(X_train_pc,columns=['PC_' +str(i) for i in range(1,pc.n_components_+1)])

"""kita panggil plt.show() untuk menampilkan hasil visualisasi"""

plt.figure(figsize=(12,6))
plt.plot(PC_df_train.std())
plt.title('Scree Plot - PCA components')
plt.xlabel('Principal Component')
plt.xticks(rotation=90)
plt.ylabel('Standard deviation')
plt.show()

"""kita ketahui bahwa terbentuk siku pada PC_15, jadi kita dapat menggunakan 10 principal components untuk analisis"""

pc = PCA(n_components=15)
X_train_pc=pc.fit_transform(Scaled_X_train)
PC_df_train=pd.DataFrame(X_train_pc,columns=['PC_' +str(i) for i in range(1,pc.n_components_+1)])

X_test_pc = pc.transform(Scaled_X_test)
PC_df_test=pd.DataFrame(X_test_pc,columns=['PC_' +str(i) for i in range(1,pc.n_components_+1)])

"""### **Modeling**

Pada tahap ini, kita akan mengembangkan model machine learning dengan tiga algoritma. Kemudian, kita akan mengevaluasi performa masing-masing algoritma dan menentukan algoritma mana yang memberikan hasil prediksi terbaik. Ketiga algoritma yang akan kita kembangkan, antara lain:

* Logistik regression
*  Support Vector Machine
*Random Forest

16. Selanjutnya adalah langkah Membuat Model. Mari kita lihat dimensi dari data train kita.
"""

print(PC_df_train.shape)
y_train.shape

"""17. Logistic Regression Model

Logistik regresion adalah metode klasifikasi pembelajaran mesin dan mencoba memprediksi variabel dependen kategoris yang dikodekan sebagai biner (1 – yes, berhasil dll., 0 – no, tidak berhasil, dll.). 

Model regresi logistik termasuk salah satu model yang sering dipakai praktisi machine learning. Regresi logistik adalah sebuah pendekatan untuk membuat model prediksi seperti halnya regresi linear atau yang biasa disebut dengan istilah Ordinary Least Squares (OLS) regression. Perbedaannya adalah pada regresi logistik, peneliti memprediksi variabel terikat yang berskala dikotomi. Skala dikotomi yang dimaksud adalah skala data nominal dengan dua kategori, misalnya: ya dan tidak, baik dan buruk, atau tinggi dan rendah. Dalam kasus nyata misalnya kita hendak memprediksi apakah individu memiliki kartu kredit ataukah tidak, berdasarkan tingkat pendapatan mereka, kita akan menggunakan regresi logistik untuk menganalisisnya. Data kepemilikan kartu kredit berbentuk kategori (memiliki dan tidak memiliki). Contoh lain, misalnya kita hendak memprediksi peranan sikap terhadap perpolitikan terhadap perilaku dalam pemilu yang dibagi menjadi dua (golput dan tidak golput).
"""

from sklearn.linear_model import LogisticRegression #import classifier
classifier = LogisticRegression()
classifier.fit(PC_df_train,y_train)
y_lr=classifier.predict(X_test_pc)

"""Mari kita tampilkan matriks evaluasi dari model yang telah kita buat"""

print('Confusion Matrix \n',confusion_matrix(y_lr,y_test))
print()
print('Accuracy Score \n', accuracy_score(y_lr,y_test))
print()
print('Classification Report \n',classification_report(y_lr,y_test))

"""18. SVC Model

*Support Vector Machine* (SVM) merupakan salah satu metode dalam supervised learning yang biasanya digunakan untuk klasifikasi (seperti Support Vector Classification) dan regresi (Support Vector Regression). Dalam pemodelan klasifikasi, SVM memiliki konsep yang lebih matang dan lebih jelas secara matematis dibandingkan dengan teknik-teknik klasifikasi lainnya. SVM juga dapat mengatasi masalah klasifikasi dan regresi dengan linear maupun non linear.

Cara kerja dari metode Support Vector Machine khususnya pada masalah non-linear adalah dengan memasukkan konsep kernel ke dalam ruang berdimensi tinggi. Tujuannya adalah untuk mencari hyperplane atau pemisah yang dapat memaksimalkan jarak (margin) antar kelas data. Untuk menemukan hyperplane terbaik, kita dapat mengukur margin kemudian mencari titik maksimalnya. Proses pencarian hyperplane yang terbaik ini adalah ini dari metode Support Vector Machine ini.
"""

from sklearn.svm import SVC #Import classifier
classifier = SVC()
classifier.fit(pc.fit_transform(X_train_pc),y_train)
y_svc=classifier.predict(X_test_pc)

"""Mari kita tampilkan matriks evaluasi dari model yang telah kita buat"""

print('Confusion Matrix \n',confusion_matrix(y_svc,y_test))
print()
print('Accuracy Score \n', accuracy_score(y_svc,y_test))
print()
print('Classification Report \n',classification_report(y_svc,y_test))

"""19. Random Forest Model

Random Forest adalah salah satu metode berbasis klasifikasi dan regresi dimana terdapat proses agregasi decision tree. Oleh karena itu, prinsip dasar random forest mirip dengan decision tree. Masing-masing decision tree akan menghasilkan output yang bisa saja berbeda-beda. Nah, random forest ini akan melakukan voting untuk menentukan hasil mayoritas dari semua decision tree. Bedanya, random forest akan memberikan output berupa mayoritas hasil dari semua decision tree. Algoritma ini memberikan akurasi yang bagus dalam klasifikasi, dapat menangani data training yang jumlahnya besar, dan juga efektif untuk mengatasi data yang tidak lengkap.
"""

from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier()
classifier.fit(X_train_pc,y_train)
y_rfc=classifier.predict(X_test_pc)

"""Mari kita tampilkan matriks evaluasi dari model yang telah kita buat"""

print('Confusion Matrix \n',confusion_matrix(y_rfc,y_test))
print()
print('Accuracy Score \n', accuracy_score(y_rfc,y_test))
print()
print('Classification Report \n',classification_report(y_rfc,y_test))

"""### EVALUATION

Pada matrix evaluasi, saya menggunakan beberapa parameter performa, yaitu F1, Recall, Precision, ROC AU Score, dan Accuracy.

* Accuracy : Accuracy Merupakan rasio prediksi Benar (positif dan negatif) dengan keseluruhan data. Akurasi menjawab pertanyaan “Berapa persen model dapat memprediksi perusahaan yang bangkrut dan tidak bangkrut dari kesuluruhan data testing”.
* Precission : Precission merupakan rasio prediksi benar positif dibandingkan dengan keseluruhan hasil yang diprediksi positf. Precission menjawab pertanyaan “Berapa persen perusahaan yang benar-benar bangkrut dari keseluruhan perusahaan yang diprediksi bangkrut?”
* Recall (Sensitifitas): Recall merupakan rasio prediksi benar positif dibandingkan dengan keseluruhan data yang benar positif. Recall menjawab pertanyaan “Berapa persen perusahaan yang diprediksi bangkrut dibandingkan keseluruhan perusahaan yang sebenarnya Bangkrut”
* F1 Score : F1 Score merupakan perbandingan rata-rata presisi dan recall yang dibobotkan
* ROC Score : 
ROC (Receiver Operating Characteristics) adalah semacam alat ukur performance untuk classification problem dalam menentukan threshold dari suatu model.

20. Model Comparison

Pada langkah ini, kita akan coba untuk melakukan komparasi dari masing-masing model yang telah kita buat. Kita akan menampilkan nilai F1, Accuracy, Recall, Precision, ROC AUC Score
"""

lr_df = pd.DataFrame(data=[f1_score(y_test,y_lr),accuracy_score(y_test, y_lr), recall_score(y_test, y_lr), precision_score(y_test, y_lr), roc_auc_score(y_test, y_lr)], 
             columns=['Logistic Regression'], index=["F1","Accuracy", "Recall", "Precision", "ROC AUC Score"])
rf_df = pd.DataFrame(data=[f1_score(y_test,y_rfc),accuracy_score(y_test, y_rfc), recall_score(y_test, y_rfc),precision_score(y_test, y_rfc), roc_auc_score(y_test, y_rfc)], 
             columns=['Random Forest'],index=["F1","Accuracy", "Recall", "Precision", "ROC AUC Score"])
svc_df = pd.DataFrame(data=[f1_score(y_test,y_svc),accuracy_score(y_test, y_svc), recall_score(y_test, y_svc), precision_score(y_test, y_svc), roc_auc_score(y_test,y_svc)], 
             columns=['SVC'], index=["F1","Accuracy", "Recall", "Precision", "ROC AUC Score"])


df_models = round(pd.concat([lr_df,rf_df,svc_df], axis=1),3)
colors = ["bisque","ivory","sandybrown","steelblue","lightsalmon"]
colormap = matplotlib.colors.LinearSegmentedColormap.from_list("", colors)

background_color = "white"

fig = plt.figure(figsize=(18,26)) # create figure
gs = fig.add_gridspec(4, 2)
gs.update(wspace=0.1, hspace=0.5)
ax0 = fig.add_subplot(gs[0, :])

sns.heatmap(df_models.T, cmap=colormap,annot=True,fmt=".1%",vmin=0,vmax=0.95, linewidths=2.5,cbar=False,ax=ax0,annot_kws={"fontsize":16})
fig.patch.set_facecolor(background_color) # figure background color
ax0.set_facecolor(background_color) 

ax0.text(0,-0.5,'Model Comparison',fontsize=20,fontweight='bold',fontfamily='serif')
plt.show()

"""Dari perbandingan diatas, dapat kita ambil kesimpulan bahwa metode Random Forest memiliki nilai akurasi, F1, Recall, Precission, dan ROC AUC Score tertinggi.sehingga dari ketiga model diatas, Random forest adalah metode terbaik untuk permaslaahan ini."""